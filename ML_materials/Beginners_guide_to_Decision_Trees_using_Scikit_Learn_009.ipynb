{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginner's guide to Decision Trees using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be taking the iris dataset from sklearn to implement DecisionTreeClassifier. Let's first import all the things necessary for running the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and testing data using the train_test_split()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(criterion='entropy',max_depth=4)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = clf.predict(x_train)\n",
    "y_test_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the result using confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[37,  0,  0],\n",
       "       [ 0, 34,  0],\n",
       "       [ 0,  1, 40]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  0,  0],\n",
       "       [ 0, 15,  1],\n",
       "       [ 0,  0,  9]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important features in decision tree classifier:\n",
    "**Entropy**<br>\n",
    "It actually decides how a decision tree should split its data.<br>\n",
    "A decision tree is built from the top-down approach where the root node is divided into subsets till we get elements with similar values (pure node).<br>\n",
    "Entropy specifies the homogeneity of the sample like how pure is the node.\n",
    "If it's completely pure then the entropy is zero.<br>\n",
    "\n",
    "**Information gain**<br>\n",
    "This value is about decreasing the entropy as much as possible after the dataset is split on a particular attribute.<br>\n",
    "For a split, we will be considering the attribute or condition with the maximum information gain possible.<br>\n",
    "\n",
    "**Split Number**<br>\n",
    "It determines the number of splits happening in a node i.e if the number of splits is more then more will be the split number.<br>\n",
    "Gain ratio<br>\n",
    "It is the ratio of information gain and the split number. The gain ratio has to be as more as possible.<br>\n",
    "\n",
    "**Gini Index**<br>\n",
    "It indicates the probability that the selected feature that the node is divided with is wrong or right. If it's right its value is zero. The low value of Gini index is desirable.<br>\n",
    "\n",
    "The Gini index varies between values 0 and 1, where 0 expresses the purity of classification, i.e. all the elements belong to a specified class or only one class exists there. And 1 indicates the random distribution of elements across various classes. The value of 0.5 of the Gini Index shows an equal distribution of elements over some classes.<br>\n",
    "\n",
    "**Difference between using Gini Index and Information Gain:**<br>\n",
    "Gini index operates on the categorical target variables in terms of \"success\" or \"failure\" and performs only binary split, in opposite to that Information Gain computes the difference between entropy before and after the split and indicates the impurity in classes of elements.<br>\n",
    "\n",
    "This method makes the algorithm reach zero errors on the training data but at the cost of reducing the accuracy of the training data. This condition is called over-fitting.<br>\n",
    "To avoid overfitting we have to implement more stopping conditions for the tree. Right now, the only two stopping criteria we have is when we reach a pure node, or when a number of features exceed a certain limit. By doing this, we stop early and avoid the building of the entire tree before it perfectly classifies the training set.<br>\n",
    "\n",
    "This can be done in two simple ways:<br>\n",
    "\n",
    "**Defining max depth(k):**<br>\n",
    "We shall decide the depth till which the tree builds its node and after that it stops.<br>\n",
    "Choosing the optimal value for k is important.<br>\n",
    "Another problem with this method is that, in some cases, we require unbalanced trees for our classification. But selecting the k value would restrict us towards building only balanced decision trees, thus wasting computation time, as well as leading to unwanted errors in classification.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3dfac027262927982790583fc11aace72addb35d60841fa2981b59a23fc0f5c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
