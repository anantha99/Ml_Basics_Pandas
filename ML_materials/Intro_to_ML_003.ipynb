{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to machine learning\n",
    "The most basic premise of ML is to build algorithm that can receive input and use different methods to predict the expected output and become more accurate as more data is availible. <br>\n",
    "\n",
    "## Applications of ML:\n",
    "Image Recognition<br>\n",
    "Speech Recognition <br>\n",
    "Traffic prediction <br>\n",
    "Self-driving cars <br>\n",
    "\n",
    "## Types of Machine Learning:\n",
    "#### 1)Supervised Learning:\n",
    "It is an approach where we try to model with well-labelled data. we then test the model by giving it a new input to predict the output.\n",
    "##### Types of Supervised Learning:\n",
    "##### Classification: \n",
    "These kinds of algorithms are used to classify the data as spam or not spam, True or False. <br>\n",
    "##### Regression : \n",
    "These kinds of algorithms want to predict continuous data or particular data like predicting the price of a product. <br>\n",
    "#### 2)Unsupervised learning:\n",
    "Unsupervised learning is a learning method in which a machine learns without any supervision.\n",
    "The training is provided to the machine with the set of data that has not been labelled, classified, or categorized, and the algorithm needs to act on that data without any supervision. The goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns.<br>\n",
    "Examples: <br>\n",
    "Feature selection <br>\n",
    "Anomaly detection<br>\n",
    "Pattern recognition <br>\n",
    "#### 3)Reinforcement learning:\n",
    "Reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action. The agent learns automatically with this feedback and improves its performance. In reinforcement learning, the agent interacts with the environment and explores it. The goal of an agent is to get the most reward points, and hence, it improves its performance.<br>\n",
    "Examples:<br>\n",
    "Robotics <br>\n",
    "Text Mining <br>\n",
    "Trade execution <br>\n",
    "\n",
    "We will be looking at only supervised learning using a dataset from sklearn. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps involved in predicting in ML <br>\n",
    "Step1:\n",
    "Gathering Data: firstly we have to get proper data became the more data we have the better our predictive model will be.<br>\n",
    "Step2: \n",
    "Data preprocessing: this process involves Cleaning the data, eliminating the missing value, and conversion of data if required. <br>\n",
    "Step3:Training the data: The dataset will be split into 2 parts to train and test. The training data will be used to build and analyse the data.<br>\n",
    "Step4: Evaluation: After building a model we use the testing data to predict the output and compare the output with the output that we already have.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Files \n",
    "We will be using the dataset example from sklearn the Boston dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analzing the data\n",
    "Once the dataset is imported it is analyzed to what the dataset contains and the dataframe is formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       " 'filename': 'C:\\\\Users\\\\Ananthapadmanabha\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\boston_house_prices.csv'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = datasets.load_boston()\n",
    "boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "        4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "        9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "        4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        7.8800e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #importing the boston dataset \n",
    "x = boston.data   #x will be a 2d dataset\n",
    "y = boston.target #y should always be one dimensional\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formation of DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "..       ...     ...    ...  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#formation of a dataframe\n",
    "c = pd.DataFrame(x,y)\n",
    "c.columns = boston.feature_names #the column names taken to be the feature names that are present\n",
    "c.reset_index(drop=True,inplace=True)\n",
    "#reset_index is used to provide continuous numbering to the dataset\n",
    "#drop is used to permanently drop the old indexing from the dataset\n",
    "#inplace is used to make the changes in the original dataset\n",
    "c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate approach (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIM</td>\n",
       "      <td>ZN</td>\n",
       "      <td>INDUS</td>\n",
       "      <td>CHAS</td>\n",
       "      <td>NOX</td>\n",
       "      <td>RM</td>\n",
       "      <td>AGE</td>\n",
       "      <td>DIS</td>\n",
       "      <td>RAD</td>\n",
       "      <td>TAX</td>\n",
       "      <td>PTRATIO</td>\n",
       "      <td>B</td>\n",
       "      <td>LSTAT</td>\n",
       "      <td>MEDV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.9</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.9</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.12</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21</td>\n",
       "      <td>396.9</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21</td>\n",
       "      <td>396.9</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.03</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.505</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>21</td>\n",
       "      <td>396.9</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>507 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0       CRIM  ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0       CRIM  ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO   \n",
       "1    0.00632  18   2.31     0  0.538  6.575  65.2    4.09    1  296     15.3   \n",
       "2    0.02731   0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "3    0.02729   0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "4    0.03237   0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "..       ...  ..    ...   ...    ...    ...   ...     ...  ...  ...      ...   \n",
       "502  0.06263   0  11.93     0  0.573  6.593  69.1  2.4786    1  273       21   \n",
       "503  0.04527   0  11.93     0  0.573   6.12  76.7  2.2875    1  273       21   \n",
       "504  0.06076   0  11.93     0  0.573  6.976    91  2.1675    1  273       21   \n",
       "505  0.10959   0  11.93     0  0.573  6.794  89.3  2.3889    1  273       21   \n",
       "506  0.04741   0  11.93     0  0.573   6.03  80.8   2.505    1  273       21   \n",
       "\n",
       "0         B  LSTAT  MEDV  \n",
       "0         B  LSTAT  MEDV  \n",
       "1     396.9   4.98    24  \n",
       "2     396.9   9.14  21.6  \n",
       "3    392.83   4.03  34.7  \n",
       "4    394.63   2.94  33.4  \n",
       "..      ...    ...   ...  \n",
       "502  391.99   9.67  22.4  \n",
       "503   396.9   9.08  20.6  \n",
       "504   396.9   5.64  23.9  \n",
       "505  393.45   6.48    22  \n",
       "506   396.9   7.88  11.9  \n",
       "\n",
       "[507 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alternate method to import the file and filtering\n",
    "original_data = pd.read_csv(boston.filename)\n",
    "original_data.columns = original_data.iloc[0]\n",
    "original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data set is organized and ready it is now time to divide the dataset into training and testing datasets for checking the model accuracy once trained.<br>\n",
    "model_selection.train_test_split() from Modelselection sub-module of sklearn is used to divide the dataset into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train,X_test,Y_train,Y_test = model_selection.train_test_split(x,y)\n",
    "#X_train - contains all the x variables that can be used for training \n",
    "#X_test - contains all the x variables that can be used for testing \n",
    "#Y_train - contains all the y variables that can be used for training \n",
    "#Y_test - contains all the y variables that can be used for testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using LinearRegression algorithm to train the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression #importing linear regression\n",
    "algo1 = LinearRegression() # creating a linear regression for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "algo1.fit(X_train,Y_train)\n",
    "#fit() is used where the algorithm trains itself to find the relationship between X_train and Y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18.81279355,  6.66005674, 17.36357432,  2.09541464, 36.27621246,\n",
       "       -1.13605148, 33.71070466, 16.95765552, 16.24055697, 16.19425743,\n",
       "       12.77673175, 28.99726487, 16.06910062, 30.5315197 , 27.99880101,\n",
       "       14.93029521, 21.59177728,  9.27753563, 16.16692899, 18.22573341,\n",
       "       24.38931068,  8.6420643 , 26.28950136, 29.75342896, 23.77422678,\n",
       "       15.23823843, 23.26255432, 26.62244227, 12.53752105, 13.80346076,\n",
       "       13.80132375, 25.38110803, 13.93493511, 20.89074382, 30.30780771,\n",
       "        7.94660865, -0.15950422, 24.22967377, 12.63414548, 37.10550801,\n",
       "       22.08788844, 12.98856607, 35.15638756, 12.54227041, 35.8107969 ,\n",
       "       34.43111518, 26.28954195,  7.6235346 , 31.16526613, 16.79827019,\n",
       "       26.36502374, 22.87831088, 32.05076756, 34.56589617, 14.71194274,\n",
       "       16.15328172, 18.38742273, 18.82588118, 16.57483999, 21.48080513,\n",
       "       30.44115282, 21.30404315,  9.26635196, 13.19843197, 30.26227834,\n",
       "       22.98784942, 21.14346338, 21.46545499, 28.75703523, 19.36828207,\n",
       "       17.73593124, 15.90634149, 20.53017271, 21.07509555, 14.30877329,\n",
       "       30.43021993, 30.93486301, 17.00117031, 27.71873561, 13.43491509,\n",
       "       28.72625031, 20.4367672 , 19.21252259, 21.57930566, 29.76351494,\n",
       "       36.34793511,  8.11484082, 10.79454897, 20.75816926, 20.23221485,\n",
       "       31.16344803, 31.63304265, 24.71257887, 16.89523691, 19.92940859,\n",
       "       12.6119606 , 34.83745094, 22.71967484, 24.59678116, 28.62083451,\n",
       "       23.72940395, 23.43331026, 19.21683043, 23.48651147, 25.8831099 ,\n",
       "       20.50594097, 31.97221935, 17.11605465, 24.24688361, 12.03692901,\n",
       "       13.35688584, 33.11007296, 16.64725449, 24.02794731, 29.83377597,\n",
       "       27.7011884 , 27.59930582, 24.75854204, 19.88421541, 19.86692248,\n",
       "       21.9885657 ,  6.54786627, 22.39231431, 33.94970398, 28.96817606,\n",
       "       31.50775916, 16.66547065])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = algo1.predict(X_test) \n",
    "#we use the predict() to predict the y values once the training is done.\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the output obtained using the graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We are using pyplot to get graphs.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(Y_test,y_predict)\n",
    "plt.axis([0,40,0,40])\n",
    "plt.show()\n",
    "#the ponits are pretty scattered from the linear line so we can conclude that the model is so much trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "The points are pretty scattered from the linear line so we can conclude that the model is not so much trained.\n",
    "We will be discussing the parameters to measure the efficiency and how we can improve the output in our next blog."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f59b2feca0a45c6a03cd5bfa15ff2c8f5d6e0dbf5634261bb3d8470f1d1b276"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
