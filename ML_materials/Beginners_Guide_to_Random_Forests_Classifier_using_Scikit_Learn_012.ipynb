{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "It combines multiple decision trees and merges them together to get a more accurate and stable prediction.<br>\n",
    "Decision trees have the problem of overfitting. Although we can use pruning to reduce it drastically. But as the number of nodes grows till we get a pure node there are more chances of overfitting.<br>\n",
    "Not all features are selected in different decision trees being made.<br>\n",
    "We try to create more decision trees so that all decision trees are included.<br>\n",
    "The randomness in selecting the features and data points helps in reducing overfitting. The final answer will be the majority of the answers from the decision trees of the forest.\n",
    "There are many ways to select the final answer. One of them is<br>\n",
    "\n",
    "## Data bagging\n",
    "\n",
    "It is a very powerful ensemble method.<br>\n",
    "An ensemble method is a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model.<br>\n",
    "Our goal is to reduce the variance of the decision trees. Combining multiple trees to give prediction will improve accuracy and precision.<br>\n",
    "\n",
    "## Procedure:\n",
    "We try to create smaller datasets in which we also repetition of data points and randomly select some features. Bagging is done generally on data points, not features.<br>\n",
    "\n",
    "These smaller data-sets are obtained by choosing the data-points and the features in the following manner:<br>\n",
    "1. Features are selected at random without repetition<br>\n",
    "2. Data-points  are selected at random with repetition (which is actually bagging)<br>\n",
    "\n",
    "\n",
    "No data points should be left out so we will try to increase the number of trees.<br>\n",
    "Selecting different features in a data set helps us know the relative importance of each feature.<br>\n",
    "\n",
    "## Feature selection\n",
    "\n",
    "Feature selection is picking only useful features that make up a major contribution to the output.<br>\n",
    "\n",
    "The advantages of feature selection are as followsÂ :<br>\n",
    "1. Reduces Overfitting<br>\n",
    "2. Improves accuracy of the model<br>\n",
    "3. Reduces training time<br>\n",
    "\n",
    "## Implementation using sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the iris dataset to see how feature selection works.<br>\n",
    "Lets import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets,tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the iris dataset and initialize 'X' to features and 'Y' to labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "features = iris.feature_names\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the RandomForestClassifier to train the model. The number of decision trees to be formed will be initialized to 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10000, n_jobs=-1, random_state = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10000, n_jobs=-1, random_state=14)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>petal width (cm)</th>\n",
       "      <td>0.459124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petal length (cm)</th>\n",
       "      <td>0.400622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <td>0.118110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <td>0.022145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   importance\n",
       "petal width (cm)     0.459124\n",
       "petal length (cm)    0.400622\n",
       "sepal length (cm)    0.118110\n",
       "sepal width (cm)     0.022145"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(clf.feature_importances_, index = features, columns=['importance']).sort_values('importance', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that <b>petal length</b> and <b>petal width</b> are important features as compared to the other two features i.e. <b>sepal length</b> and <b>sepal width</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a classifier picking only important features, \n",
    "# picking only those features that have importance value greater than 0.15\n",
    "sfm = SelectFromModel(clf, threshold = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(n_estimators=10000, n_jobs=-1,\n",
       "                                                 random_state=14),\n",
       "                threshold=0.15)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data subset picking only important features out of all the features.\n",
    "X_important_train = sfm.transform(X_train)\n",
    "X_important_test = sfm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New random forest classifier with only important features\n",
    "clf_important = RandomForestClassifier(n_estimators=10000, n_jobs=-1, random_state = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit this with the important features with which we created a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10000, n_jobs=-1, random_state=14)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_important.fit(X_important_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the score which tells us how the model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_important.score(X_important_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3dfac027262927982790583fc11aace72addb35d60841fa2981b59a23fc0f5c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
